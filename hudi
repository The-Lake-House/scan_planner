#!/usr/bin/env python

import argparse
import os
import re
import sys

import pymetastore.metastore

import utils

parser = argparse.ArgumentParser()
parser.add_argument('database_name')
parser.add_argument('table_name')
parser.add_argument('--hms-host', default='localhost')
parser.add_argument('--hms-port', default=9083)
args = parser.parse_args()

# [File Id]_[File Write Token]_[Transaction timestamp].[File Extension]
base_file_regex = re.compile(r'([0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12})-[0-9]_[^_]+_(\d+)\..*')

# [File Id]_[Base Transaction timestamp].[Log File Extension].[Log File Version]_[File Write Token]
log_file_regex = re.compile(r'\.([0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12})-[0-9]_(\d+)\.log\..*')

with pymetastore.metastore.HMS.create(host=args.hms_host, port=args.hms_port) as hms:
    try:
        table = hms.get_table(
            database_name=args.database_name,
            table_name=args.table_name,
        )
    except pymetastore.hive_metastore.ttypes.NoSuchObjectException:
        print(f'Table {args.table_name} does not exist', file=sys.stderr)
        sys.exit(1)
    s3_bucket, s3_prefix = utils.parse_s3a(table.storage.location)
    partition_key = None
    for line in utils.s3_get(s3_bucket, s3_prefix + '/.hoodie/hoodie.properties').decode().splitlines():
        if 'hoodie.table.partition.fields' in line:
            partition_key = line.split('=')[1]
    commit_files_list = utils.s3_list_objects(s3_bucket, s3_prefix + '/.hoodie/')
    newest_commit_path = max(commit_files_list, key=lambda x: x['LastModified'])['Key']
    timestamp = os.path.splitext(os.path.basename(newest_commit_path))[0]
    partition_dirs = []
    if partition_key:
        for subdir in utils.s3_list_subdirs(s3_bucket, s3_prefix + '/'):
            if partition_key in subdir['Prefix']:
                partition_dirs.append(subdir['Prefix'])
    else:
        partition_dirs.append(s3_prefix + '/')
    data_files = {}
    for partition_dir in partition_dirs:
        data_file_list = [os.path.basename(data_file['Key']) for data_file in utils.s3_list_objects(s3_bucket, partition_dir)]
        for data_file in data_file_list:
            # Process base file
            base_file_match = base_file_regex.match(data_file)
            if base_file_match:
                file_id, file_timestamp = base_file_match.groups()
                if file_timestamp <= timestamp:
                    if file_id not in data_files:
                        data_files[file_id] = []
                    data_files[file_id].append({
                        'type': 'base',
                        'timestamp': file_timestamp,
                        'path': partition_dir + data_file
                    })
            # Process log file
            log_file_match = log_file_regex.match(data_file)
            if log_file_match:
                file_id, file_timestamp = log_file_match.groups()
                if file_timestamp <= timestamp:
                    if file_id not in data_files:
                        data_files[file_id] = []
                    data_files[file_id].append({
                        'type': 'log',
                        'timestamp': file_timestamp,
                        'path': partition_dir + data_file
                    })
    # Sort file ID entries by timestamp
    data_files = {key: sorted(value, key=lambda x: x['timestamp'], reverse=True) for key, value in data_files.items()}
    # Print data files
    for file_id, file_obj_list in data_files.items():
        for file_obj in file_obj_list:
            # Stop after first base file
            if file_obj['type'] == 'base':
                print(f"s3a://{s3_bucket}/{file_obj['path']}")
                break
            else:
                print(f"s3a://{s3_bucket}/{file_obj['path']}")
